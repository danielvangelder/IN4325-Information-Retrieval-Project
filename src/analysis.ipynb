{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6517e324ec9dcf41a34bac95748c49ce676fe8f2d7422e34b5859cd3f859ad06"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions we aim to answer in the analysis\n",
    "# 1) Behavior on top relevant documents [How many of the top documents for this system were relevant and could they be categorized and distinguished from others?]\n",
    "# 2) Behavior on top non-relevant documents [Why were the top non-relevant documents retrieved?] Behavior on unretrieved relevant documents [Why weren’t these relevant documents retrieved within the top 1000?]\n",
    "# x) Beadplot observations [How does the ranking (especially among the top 50 documents) of this system compare to all other systems?]\n",
    "# 3) Base Query observations [What did the system think were the important terms of the original query, and were they good?]\n",
    "# 4) Expanded Query observations [If the system expanded the query (4 out of 6 systems did), what were the important terms of the expansion, and were they helpful?]\n",
    "# 5) Blunders of system [What obvious mistakes did the system make that it could have easily avoided? Examples might be bad stemming of words or bad handling of hyphenation] Other features of note [Anything else.]\n",
    "# 6) What should system to do improve performance? [The individual’s conclusion as to why the system did not retrieve well, and recommendations as to what would have made a better retrieval.]\n",
    "# 7) What added information would help performance? How can system get that information? [Is there implicit information in the query, that a human would understand but the system didn’t? Examples might be world knowledge (like Germany is part of Europe).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reads the output of a model.\n",
    "# Lines in the output should be in the form [query_id, doc_id, rank] with sep = '\\t'.\n",
    "def read_results_tsv(loc):\n",
    "    d = pd.read_csv(loc, sep='\\t', header=None, names=['query_id', 'doc_id', 'rank', 'score'])\n",
    "    return d\n",
    "\n",
    "# Reads the output of a model.\n",
    "# Lines in the output should be in the form [query_id, doc_id, rank] with sep=' '.\n",
    "def read_results_csv(loc):\n",
    "    d = pd.read_csv(loc, sep=' ', header=None, names=['query_id', 'doc_id', 'rank', 'score'])\n",
    "    return d\n",
    "\n",
    "# Gets the ranking of a query.\n",
    "def get_ranking_by_query_id(d, query_id):\n",
    "    ranking = d.loc[d.query_id == query_id][['doc_id', 'rank']].sort_values(by=['rank'])['doc_id'].tolist()\n",
    "    return ranking\n",
    "\n",
    "d = read_results_csv(\"data/bm25-msmarco-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reads the relevant documents from the given qrels file.\n",
    "def read_qrels(loc):\n",
    "    d = pd.read_csv(loc, names=['query_id', 'Q0', 'doc_id', 'rating'], sep=' ', header=None)\n",
    "    del d['Q0']\n",
    "    return d\n",
    "\n",
    "# Gets the relevant document for the given query id.\n",
    "def get_relevant_doc_ids(qrels, query_id):\n",
    "    doc_ids = qrels.loc[(qrels.query_id == query_id) & (qrels.rating >= 2)][['doc_id', 'rating']]\n",
    "    return doc_ids\n",
    "\n",
    "def get_recall_per_query(qrels, results, n):\n",
    "    recalls = {}\n",
    "    for query_id in qrels.query_id.unique():\n",
    "        ranking = get_ranking_by_query_id(results, query_id)\n",
    "        relevant = get_relevant_doc_ids(qrels, query_id)\n",
    "        recalls[query_id] = 0\n",
    "        for i in range(0, n):\n",
    "            ratings = relevant.loc[relevant.doc_id == ranking[i]].rating.tolist()\n",
    "            if len(ratings) > 0 and ratings[0] >= 2:\n",
    "                recalls[query_id] += 1\n",
    "    return recalls\n",
    "\n",
    "# Constructs a vector which counts the number of retrieved documents for each rating.\n",
    "def get_relevance_vector(qrels, results):\n",
    "    v = np.zeros(qrels.rating.max() + 1)\n",
    "    for query_id in qrels.query_id.unique():\n",
    "        ranking = get_ranking_by_query_id(d, query_id)\n",
    "        relevant = get_relevant_doc_ids(qrels, query_id)\n",
    "        for doc_id in ranking:\n",
    "            rating = relevant.loc[relevant.doc_id == doc_id]['rating']\n",
    "            v[rating] += 1\n",
    "    return v\n",
    "\n",
    "# Given a ranking, return all documents that are relevant, but not in the ranking for the given query.\n",
    "def get_relevant_doc_ids_not_retrieved(qrels, query_id, ranking):\n",
    "    relevant_doc_ids = get_relevant_doc_ids(qrels, query_id).doc_id.tolist()\n",
    "    relevant_doc_ids_not_retrieved = []\n",
    "    for doc_id in relevant_doc_ids:\n",
    "        if not doc_id in ranking:\n",
    "            relevant_doc_ids_not_retrieved.append(doc_id)\n",
    "    return relevant_doc_ids_not_retrieved\n",
    "\n",
    "qrels = read_qrels('../anserini/collections/msmarco-passage/2019qrels-pass.txt')\n",
    "\n",
    "# For some reason, qrels contains less unique query ids, i.e., not every query has relevant items (by a long shot).\n",
    "# print(qrels.query_id.unique())\n",
    "# print(d.query_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(loc):\n",
    "    queries = pd.read_csv(loc, header=None, sep='\\t', names=['query_id', 'string'])\n",
    "    return queries\n",
    "\n",
    "def get_query(queries, query_id):\n",
    "    return queries.loc[queries.query_id == query_id].string.tolist()[0]\n",
    "\n",
    "queries = read_queries(\"data/msmarco-test2019-queries.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader\n",
    "\n",
    "index_loc = \"../anserini/indexes/msmarco-passage/lucene-index-msmarco\"\n",
    "index = IndexReader(index_loc)\n",
    "\n",
    "# Gets the document vector for the given doc_id.\n",
    "def get_doc_vec(doc_id):\n",
    "    if (type(doc_id) == type(0)):\n",
    "        return index.get_document_vector(\"{}\".format(doc_id))\n",
    "    else:\n",
    "        return index.get_document_vector(doc_id)\n",
    "\n",
    "# Tokenizes a given query.\n",
    "def tokenize(query):\n",
    "    return index.analyze(query)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "215.34883720930233\n"
     ]
    }
   ],
   "source": [
    "# There are on average ~215 documents per query in the qrels.\n",
    "# This means that a model can only retrieve on average at most 215 documents.\n",
    "print(len(qrels) / len(qrels.query_id.unique()))"
   ]
  },
  {
   "source": [
    "### 1) Behavior on top relevant documents. How many of the top documents for this system were relevant and could they be categorized and distinguished from others?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from subprocess import check_output\n",
    "import subprocess\n",
    "\n",
    "# This question can be answered using the calculating the metrics used by the official trec_eval tool.\n",
    "# https://www-nlpir.nist.gov/projects/trecvid/trecvid.tools/trec_eval_video/A.README\n",
    "cmd = subprocess.Popen(['../anserini/tools/eval/trec_eval.9.0.4/trec_eval', '-c', '-mofficial', 'data/2019qrels-pass.txt', 'data/bm25-msmarco-test.trec'], stdout=subprocess.PIPE)\n",
    "cmd_out, cmd_err = cmd.communicate()\n",
    "print(cmd_out.decode(\"utf-8\"))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 103,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "runid                 \tall\tbm25\nnum_q                 \tall\t43\nnum_ret               \tall\t43000\nnum_rel               \tall\t4102\nnum_rel_ret           \tall\t2814\nmap                   \tall\t0.3774\ngm_map                \tall\t0.2465\nRprec                 \tall\t0.3964\nbpref                 \tall\t0.5000\nrecip_rank            \tall\t0.8245\niprec_at_recall_0.00  \tall\t0.8579\niprec_at_recall_0.10  \tall\t0.6687\niprec_at_recall_0.20  \tall\t0.5796\niprec_at_recall_0.30  \tall\t0.5073\niprec_at_recall_0.40  \tall\t0.4169\niprec_at_recall_0.50  \tall\t0.3689\niprec_at_recall_0.60  \tall\t0.3138\niprec_at_recall_0.70  \tall\t0.2596\niprec_at_recall_0.80  \tall\t0.1927\niprec_at_recall_0.90  \tall\t0.1161\niprec_at_recall_1.00  \tall\t0.0340\nP_5                   \tall\t0.6930\nP_10                  \tall\t0.6186\nP_15                  \tall\t0.5798\nP_20                  \tall\t0.5453\nP_30                  \tall\t0.4953\nP_100                 \tall\t0.3198\nP_200                 \tall\t0.2266\nP_500                 \tall\t0.1176\nP_1000                \tall\t0.0654\n\n"
     ]
    }
   ]
  },
  {
   "source": [
    "We can see that of the 4102 relevant documents, BM25 managed to find 2814. Furthermore, when we look at the precision metrics, we can see that the most relevant documents are found early on, as the precision at 10 retrieved documents is 60%, but that the precision at 100 retrieved documents is only 30%. This means that BM25 cannot make a clear distinction between relevant and non-relevant documents accross the retreived documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2) Behavior on top non-relevant documents Why were the top non-relevant documents retrieved? Behavior on unretrieved relevant documents Why weren’t these relevant documents retrieved within the top 1000?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['caus', 'militari', 'suicid']\n{'half': 2, 'expos': 1, 'about': 2, 'american': 1, 'had': 1, 'injuri': 1, 'while': 1, 'riski': 1, 'when': 1, 'drug': 1, 'problem': 2, 'white': 1, 'than': 3, '52': 1, 'alcohol': 1, 'all': 2, 'trauma': 1, 'which': 1, 'like': 2, 'onli': 1, 'vietnam': 2, 'them': 1, 'develop': 1, 'seriou': 1, '1': 1, '2': 2, 'symptom': 2, '3': 1, 'million': 2, 'veteran': 3, '6': 1, '1.7': 2, '60': 1, 'women': 2, '27': 1, '28': 1, 'affect': 1, 'medic': 1, 'combat': 1, 'experienc': 2, 'seen': 1, 'abus': 2, 'men': 2, 'have': 5, 'behavior': 1, 'ptsd': 6, '35': 1, 'more': 4, 'health': 1, 'war': 1, 'also': 1, 'although': 1, 'due': 1, 'african': 1}\n"
     ]
    }
   ],
   "source": [
    "query_id = qrels.query_id.unique()[3]\n",
    "query = get_query(queries, query_id)\n",
    "ranking = get_ranking_by_query_id(d, query_id)\n",
    "relevant_doc_ids_not_retrieved = get_relevant_doc_ids_not_retrieved(qrels, query_id, ranking)\n",
    "print(tokenize(query))\n",
    "print(get_doc_vec(relevant_doc_ids_not_retrieved[0]))"
   ]
  },
  {
   "source": [
    "In this case, synonyms and similar terms for military such as 'veteran' and 'medic' could have helped in finding this document."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3) Base Query observations. What did the system think were the important terms of the original query, and were they good?\n",
    "\n",
    "BM25 has no term weighing? It only removes non-important words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4) Expanded Query observations. If the system expanded the query (4 out of 6 systems did), what were the important terms of the expansion, and were they helpful?\n",
    "\n",
    "BM25 Uses no expanded queries."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5) Blunders of system. What obvious mistakes did the system make that it could have easily avoided? Examples might be bad stemming of words or bad handling of hyphenation. Other features of note. Anything else.\n",
    "\n",
    "We can answer this question by looking at the queries with the worst recall."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "what are the three percenters?\n['what', 'three', 'percent']\n{'militia': 1, 'refus': 1, 'govern': 1, 'subject': 1, 'own': 1, '09': 1, 'american': 1, 'british': 1, 'three': 1, 'second': 2, '3per': 1, 'loos': 1, 'bear': 1, 'from': 1, 'up': 1, 'took': 1, 'law': 1, 'king': 1, 'like': 1, 'amend': 2, 'revolutionari': 1, 'patriot': 1, 'onli': 1, 'come': 1, 'carri': 1, 'dure': 1, 'disarm': 1, '3': 3, 'constitut': 1, 'keep': 1, 'firearm': 1, 'name': 1, 'octob': 1, 'who': 1, 'fact': 1, 'against': 1, 'percent': 4, 'revolut': 1, 'we': 3, 'coloni': 1, 'violat': 1, 'arm': 1, 'win': 1, 'vow': 1, 'independ': 1, 'mind': 1, 'war': 1, 'enough': 1, 'right': 1, 'compli': 2, 'affili': 1, 'popul': 1, '2013': 1, 'overthrow': 1}\nwhy did the us volunterilay enter ww1\n['why', 'did', 'us', 'volunterilai', 'enter', 'ww1']\n{\n  \"id\" : \"1048074\",\n  \"contents\" : \"Woodrow Wilson also embarked on reorganising the federal banking system. From 1914 to 1917, he observed a strict neutrality in the Great War but the activities of German U-boats forced his hand especially with the sinking of the âLusitaniaâ in 1915 which killed 128 American citizens.\"\n}\n"
     ]
    }
   ],
   "source": [
    "recalls = [(k, v) for k, v in get_recall_per_query(qrels, d, 20).items()]\n",
    "recalls.sort(key=lambda x: x[1])\n",
    "worst_query_id = recalls[0][0]\n",
    "\n",
    "print(get_query(queries, worst_query_id))\n",
    "print(tokenize(get_query(queries, worst_query_id)))\n",
    "\n",
    "relevant = get_relevant_doc_ids_not_retrieved(qrels, worst_query_id, get_ranking_by_query_id(d, worst_query_id))\n",
    "print(get_doc_vec(relevant[0]))\n",
    "\n",
    "worst_query_id = recalls[2][0]\n",
    "\n",
    "print(get_query(queries, worst_query_id))\n",
    "print(tokenize(get_query(queries, worst_query_id)))\n",
    "\n",
    "relevant = get_relevant_doc_ids_not_retrieved(qrels, worst_query_id, get_ranking_by_query_id(d, worst_query_id))\n",
    "print(index.doc(str(relevant[0])).raw())\n"
   ]
  },
  {
   "source": [
    "In this case, the problem with the query with the worst performance is that Percenter is reduced to percent, which now matches with any document using the word percent. This is an obvious stemming issue. This could be resolved by using NER information.\n",
    "\n",
    "In the second query, a spelling mistake prevents good retrieval, and again synonyms for WW1 might be useful."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 6) What should system to do improve performance? The individual’s conclusion as to why the system did not retrieve well, and recommendations as to what would have made a better retrieval."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7) What added information would help performance? How can system get that information? Is there implicit information in the query, that a human would understand but the system didn’t? Examples might be world knowledge (like Germany is part of Europe)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In general, similar terms and synonyms could benefit greatly in retrieval as queries are often times very small and might miss key terms. Also, a spell checker may benefit search as some queries showed spelling errors which prevents matching of the same intended word. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}