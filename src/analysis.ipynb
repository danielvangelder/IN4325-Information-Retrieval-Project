{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions we aim to answer in the analysis\n",
    "1. Behavior on top relevant documents [How many of the top documents for this system were relevant and could they be categorized and distinguished from others?]\n",
    "2. Behavior on top non-relevant documents [Why were the top non-relevant documents retrieved?] Behavior on unretrieved relevant documents [Why weren’t these relevant documents retrieved within the top 1000?]\n",
    "\n",
    "    x. Beadplot observations [How does the ranking (especially among the top 50 documents) of this system compare to all other systems?]\n",
    "\n",
    "3. Base Query observations [What did the system think were the important terms of the original query, and were they good?]\n",
    "4. Expanded Query observations [If the system expanded the query (4 out of 6 systems did), what were the important terms of the expansion, and were they helpful?]\n",
    "5. Blunders of system [What obvious mistakes did the system make that it could have easily avoided? Examples might be bad stemming of words or bad handling of hyphenation] Other features of note [Anything else.]\n",
    "6. What should system to do improve performance? [The individual’s conclusion as to why the system did not retrieve well, and recommendations as to what would have made a better retrieval.]\n",
    "7. What added information would help performance? How can system get that information? [Is there implicit information in the query, that a human would understand but the system didn’t? Examples might be world knowledge (like Germany is part of Europe).]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: define local paths to result files so that these can be used for analysis. If your files are in a different location or format, change the values of these constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to output file of the model in TREC format\n",
    "MODEL_OUTPUT_PATH = './LambdaRANK_resuls_01.trec'\n",
    "# Separator used in output file between values\n",
    "SEPARATOR = ' '\n",
    "# Path to MS-MARCO evaluation queries\n",
    "MSMARCO_QUERIES_PATH = 'collections/msmarco-passage/msmarco-test2019-queries.tsv' \n",
    "# Path to Qrels file of the aforementioned queries\n",
    "MSMARCO_QRELS_PATH = 'collections/msmarco-passage/2019qrels-pass.txt'\n",
    "# Path to indexes of the MSMARCO dataset\n",
    "INDEX_PATH = 'indexes/lucene-index-msmarco-passage'\n",
    "# Path to TREC evaluation file\n",
    "TREC_EVAL_PATH = 'tools/eval/trec_eval.9.0.4/trec_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reads the output of a model.\n",
    "# Lines in the output should be in the form [query_id, doc_id, rank] with sep = '\\t'.\n",
    "def read_results_tsv(loc):\n",
    "    d = pd.read_csv(loc, sep='\\t', header=None, names=['query_id', 'doc_id', 'rank', 'score'])\n",
    "    return d\n",
    "\n",
    "# Reads the output of a model.\n",
    "# Lines in the output should be in the form [query_id, doc_id, rank] with sep=' '.\n",
    "def read_results_csv(loc):\n",
    "    d = pd.read_csv(loc, sep=' ', header=None, names=['query_id', 'doc_id', 'rank', 'score'])\n",
    "    return d\n",
    "\n",
    "# Gets the ranking of a query.\n",
    "def get_ranking_by_query_id(d, query_id):\n",
    "    ranking = d.loc[d.query_id == query_id][['doc_id', 'rank']].sort_values(by=['rank'])['doc_id'].tolist()\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reads the relevant documents from the given qrels file.\n",
    "def read_qrels(loc):\n",
    "    d = pd.read_csv(loc, names=['query_id', 'Q0', 'doc_id', 'rating'], sep=' ', header=None)\n",
    "    del d['Q0']\n",
    "    return d\n",
    "\n",
    "def read_trec_results(separator, path):\n",
    "    '''Reads the results file that is in TREC format: query_id, Q0, doc_id, rank, score separates as a csv'''\n",
    "    d = pd.read_csv(path, sep=separator, header=None, names=['query_id', 'Q0', 'doc_id', 'rank', 'score', 'run_name'])\n",
    "    # Remove redundant columns\n",
    "    del d['Q0']\n",
    "    del d['run_name']\n",
    "    return d\n",
    "\n",
    "# Gets the relevant document for the given query id.\n",
    "def get_non_relevant_doc_ids(qrels, query_id):\n",
    "    doc_ids = qrels.loc[(qrels.query_id == query_id) & (qrels.rating < 2)][['doc_id', 'rating']]\n",
    "    return doc_ids\n",
    "\n",
    "# Gets the relevant document for the given query id.\n",
    "def get_relevant_doc_ids(qrels, query_id):\n",
    "    doc_ids = qrels.loc[(qrels.query_id == query_id) & (qrels.rating >= 2)][['doc_id', 'rating']]\n",
    "    return doc_ids\n",
    "\n",
    "def get_recall_per_query(qrels, results, n):\n",
    "    recalls = {}\n",
    "    for query_id in qrels.query_id.unique():\n",
    "        ranking = get_ranking_by_query_id(results, query_id)\n",
    "        relevant = get_relevant_doc_ids(qrels, query_id)\n",
    "        recalls[query_id] = 0\n",
    "        for i in range(0, n):\n",
    "            ratings = relevant.loc[relevant.doc_id == ranking[i]].rating.tolist()\n",
    "            if len(ratings) > 0 and ratings[0] >= 2:\n",
    "                recalls[query_id] += 1\n",
    "    return recalls\n",
    "\n",
    "# Constructs a vector which counts the number of retrieved documents for each rating.\n",
    "def get_relevance_vector(qrels, results):\n",
    "    v = np.zeros(qrels.rating.max() + 1)\n",
    "    for query_id in qrels.query_id.unique():\n",
    "        ranking = get_ranking_by_query_id(d, query_id)\n",
    "        relevant = get_relevant_doc_ids(qrels, query_id)\n",
    "        for doc_id in ranking:\n",
    "            rating = relevant.loc[relevant.doc_id == doc_id]['rating']\n",
    "            v[rating] += 1\n",
    "    return v\n",
    "\n",
    "# Given a ranking, return all documents that are relevant, but not in the ranking for the given query.\n",
    "def get_relevant_doc_ids_not_retrieved(qrels, query_id, ranking):\n",
    "    relevant_doc_ids = get_relevant_doc_ids(qrels, query_id).doc_id.tolist()\n",
    "    relevant_doc_ids_not_retrieved = []\n",
    "    for doc_id in relevant_doc_ids:\n",
    "        if not doc_id in ranking:\n",
    "            relevant_doc_ids_not_retrieved.append(doc_id)\n",
    "    return relevant_doc_ids_not_retrieved\n",
    "\n",
    "# Given a ranking, return all documents that are relevant and are in the ranking for the given query.\n",
    "def get_relevant_doc_ids_retrieved(qrels, query_id, ranking):\n",
    "    relevant_doc_ids = get_relevant_doc_ids(qrels, query_id).doc_id.tolist()\n",
    "    relevant_doc_ids_retrieved = []\n",
    "    for doc_id in relevant_doc_ids:\n",
    "        if doc_id in ranking:\n",
    "            relevant_doc_ids_retrieved.append(doc_id)\n",
    "    return relevant_doc_ids_retrieved\n",
    "\n",
    "# Given a ranking, return all documents that are non-relevant and are in the ranking for the given query.\n",
    "def get_non_relevant_doc_ids_retrieved(qrels, query_id, ranking):\n",
    "    non_relevant_doc_ids = get_non_relevant_doc_ids(qrels, query_id).doc_id.tolist()\n",
    "    non_relevant_doc_ids_retrieved = []\n",
    "    for doc_id in non_relevant_doc_ids:\n",
    "        if doc_id in ranking:\n",
    "            non_relevant_doc_ids_retrieved.append(doc_id)\n",
    "    return non_relevant_doc_ids_retrieved\n",
    "\n",
    "\n",
    "\n",
    "# For some reason, qrels contains less unique query ids, i.e., not every query has relevant items (by a long shot).\n",
    "# print(qrels.query_id.unique())\n",
    "# print(d.query_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(loc):\n",
    "    queries = pd.read_csv(loc, header=None, sep='\\t', names=['query_id', 'string'])\n",
    "    return queries\n",
    "\n",
    "def get_query(queries, query_id):\n",
    "    return queries.loc[queries.query_id == query_id].string.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader\n",
    "\n",
    "# Load datafiles:\n",
    "d = read_trec_results(SEPARATOR, MODEL_OUTPUT_PATH)\n",
    "qrels = read_qrels(MSMARCO_QRELS_PATH)\n",
    "queries = read_queries(MSMARCO_QUERIES_PATH)\n",
    "index = IndexReader(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the document vector for the given doc_id.\n",
    "def get_doc_vec(doc_id):\n",
    "    if (type(doc_id) == type(0)):\n",
    "        return index.get_document_vector(\"{}\".format(doc_id))\n",
    "    else:\n",
    "        return index.get_document_vector(doc_id)\n",
    "\n",
    "# Tokenizes a given query.\n",
    "def tokenize(query):\n",
    "    return index.analyze(query)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "215.34883720930233\n"
     ]
    }
   ],
   "source": [
    "# There are on average ~215 documents per query in the qrels.\n",
    "# This means that a model can only retrieve on average at most 215 documents.\n",
    "print(len(qrels) / len(qrels.query_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Behavior on top relevant documents. How many of the top documents for this system were relevant and could they be categorized and distinguished from others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "runid                 \tall\tbm25\nnum_q                 \tall\t43\nnum_ret               \tall\t43000\nnum_rel               \tall\t4102\nnum_rel_ret           \tall\t2814\nmap                   \tall\t0.3774\ngm_map                \tall\t0.2465\nRprec                 \tall\t0.3964\nbpref                 \tall\t0.5000\nrecip_rank            \tall\t0.8245\niprec_at_recall_0.00  \tall\t0.8579\niprec_at_recall_0.10  \tall\t0.6687\niprec_at_recall_0.20  \tall\t0.5796\niprec_at_recall_0.30  \tall\t0.5073\niprec_at_recall_0.40  \tall\t0.4169\niprec_at_recall_0.50  \tall\t0.3689\niprec_at_recall_0.60  \tall\t0.3138\niprec_at_recall_0.70  \tall\t0.2596\niprec_at_recall_0.80  \tall\t0.1927\niprec_at_recall_0.90  \tall\t0.1161\niprec_at_recall_1.00  \tall\t0.0340\nP_5                   \tall\t0.6930\nP_10                  \tall\t0.6186\nP_15                  \tall\t0.5798\nP_20                  \tall\t0.5453\nP_30                  \tall\t0.4953\nP_100                 \tall\t0.3198\nP_200                 \tall\t0.2266\nP_500                 \tall\t0.1176\nP_1000                \tall\t0.0654\nrecall_5              \tall\t0.0838\nrecall_10             \tall\t0.1285\nrecall_15             \tall\t0.1671\nrecall_20             \tall\t0.2014\nrecall_30             \tall\t0.2627\nrecall_100            \tall\t0.4537\nrecall_200            \tall\t0.5603\nrecall_500            \tall\t0.6821\nrecall_1000           \tall\t0.7389\n\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "import subprocess\n",
    "\n",
    "# This question can be answered using the calculating the metrics used by the official trec_eval tool.\n",
    "# https://www-nlpir.nist.gov/projects/trecvid/trecvid.tools/trec_eval_video/A.README\n",
    "cmd = subprocess.Popen([TREC_EVAL_PATH, '-c', '-mofficial', '-mrecall', MSMARCO_QRELS_PATH, MODEL_OUTPUT_PATH], stdout=subprocess.PIPE)\n",
    "cmd_out, cmd_err = cmd.communicate()\n",
    "print(cmd_out.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 Results\n",
    "We can see that of the 4102 relevant documents, BM25 managed to find 2814. Furthermore, when we look at the precision metrics, we can see that the most relevant documents are found early on, as the precision at 10 retrieved documents is 60%, but that the precision at 100 retrieved documents is only 30%. This means that BM25 cannot make a clear distinction between relevant and non-relevant documents accross the retreived documents.\n",
    "\n",
    "#### MonoT5 Results\n",
    "(_note that these results are for a k=100, that means only 100 documents are retrieved for a query_)\n",
    "Of the 4102 relevant documents, 916 were retrieved over all queries (1372 of the 4300 retrieved documents were relevant). Observe that the precision is very high over the first few documents (<10) but decreases significantly as more documents are retrieved:\n",
    "\n",
    "|Rank|Precision|\n",
    "|---|---|\n",
    "|5   |0.8977|\n",
    "|10  |0.8070|\n",
    "|15  | 0.7380|\n",
    "|20  |0.6930|\n",
    "|30  |0.6217|\n",
    "|100 |0.3191|\n",
    "\n",
    "Also notice that the mean reciprocal rank is very high (> 0.97). MAP is very low (~ 0.37) but that is probably due to the low amount of documents retrieved by the model (`k=100`) while there could be more than 200 relevant documents for a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Behavior on top non-relevant documents Why were the top non-relevant documents retrieved? Behavior on unretrieved relevant documents Why weren’t these relevant documents retrieved within the top 1000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['caus', 'militari', 'suicid']\n['ptsd', 'have', 'more', 'than', 'veteran', 'half', 'about', 'problem', 'all', 'like']\n"
     ]
    }
   ],
   "source": [
    "i=17\n",
    "\n",
    "query_id = qrels['query_id'].unique()[i]\n",
    "print(query_id)\n",
    "for t, q_id in enumerate(qrels['query_id'].unique()):\n",
    "    if \"law\" in get_query(queries, q_id):\n",
    "        print(t, q_id)\n",
    "query = get_query(queries, query_id)\n",
    "ranking = get_ranking_by_query_id(d, query_id)\n",
    "print(tokenize(query))\n",
    "# Functions added:\n",
    "#get_non_relevant_doc_ids_retrieved\n",
    "#get_relevant_doc_ids_retrieved\n",
    "#get_relevant_doc_ids_not_retrieved\n",
    "docs = get_relevant_doc_ids_not_retrieved(qrels, query_id, ranking)\n",
    "doc_vec = [get_doc_vec(doc) for doc in docs]\n",
    "# Print 10 most occuring words in document\n",
    "for i in range(len(doc_vec)):\n",
    "    print(i, docs[i], [k for k, v in sorted(doc_vec[i].items(), key=lambda item: item[1], reverse=True)[:10]])\n",
    "#     print(i, ranking.index(docs[i]), docs[i], [k for k, v in sorted(doc_vec[i].items(), key=lambda item: item[1], reverse=True)[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130510 definition declaratory judgment\n",
      "0 0 1 2 1494936 ['declaratori', 'judgment', 'parti', 'legal', 'involv', 'sometim', 'conclus', 'appeal', 'right', 'court']\n",
      "4 0 2 3 8612906 ['declaratori', 'judgment', 'state', 'disput', 'fact', 'patent', 'piec', 'own', 'properti', 'rule']\n",
      "5 0 3 3 8612903 ['definit', 'other', 'rule', 'parti', 'declaratori', 'award', 'judgment', 'rate', 'civil', 'damag']\n",
      "6 0 4 3 799647 ['judgment', 'ani', 'parti', 'declaratori', 'case', 'disput', 'duti', 'right', 'court', 'howev']\n",
      "10 0 5 2 1494935 ['declaratori', 'act', 'judgment', 'mai', 'injunct', 'advoc', 'procedur', 'statutori', 'provis', 'seek']\n",
      "11 0 6 2 1494938 ['judgment', 'declareâ', 'â', 'featur', 'author', 'forc', 'it', 'right', 'parti', 'declaratori']\n",
      "13 0 7 3 8612909 ['legal', 'call', 'determin', 'judgment', 'resolv', 'litig', 'declar', 'also', 'court', 'uncertainti']\n",
      "15 0 8 3 8612910 ['which', 'law', 'question', 'anyth', 'express', 'right', 'court', 'done', 'parti', 'simpli']\n",
      "17 0 9 3 8612902 ['other', 'natur', 'anyth', 'do', 'court', 'howev', 'matter', 'requir', 'parti', 'without']\n",
      "23 0 10 2 8514425 ['action', 'titl', 'other', 'brought', 'against', 'properti', 'him', 'priviti', 'declaratori', 'cloud']\n",
      "24 0 11 2 8612904 ['remedi', 'equit', 'thu', 'though', 'gener', 'subject', 'statutori', 'court', 'requir', 'declaratori']\n",
      "30 0 12 2 7501560 ['action', 'â', 'organ', 'exempt', 'gift', 'instal', 'valuat', 'declaratori', 'gov', 'mai']\n",
      "38 0 13 2 1494939 ['controversi', 'rather', 'hypothet', 'parti', 'simpli', 'declaratori', 'jurisdict', 'controversiesâ', 'fall', 'judgment']\n",
      "121 0 14 2 2982053 ['remedi', 'what', 'individu', 'awar', 'main', 'when', 'declaratori', 'wish', 'determin', 'whether']\n"
     ]
    }
   ],
   "source": [
    "i=5\n",
    "query_id = qrels['query_id'].unique()[i]\n",
    "print(query_id, get_query(queries, query_id))\n",
    "query = get_query(queries, query_id)\n",
    "ranking = get_ranking_by_query_id(d, query_id)\n",
    "j = 0\n",
    "p = 0\n",
    "for i, doc_id in enumerate(ranking):\n",
    "    rating = qrels.loc[(qrels.query_id == query_id) & (qrels.doc_id == doc_id)][['rating']].values\n",
    "    if len(rating) == 0:\n",
    "        rating = 0\n",
    "    else:\n",
    "        rating = rating[0][0]\n",
    "    \n",
    "    \n",
    "    if rating > 1:\n",
    "        j += 1\n",
    "        top_10 = [k for k, v in sorted(get_doc_vec(doc_id).items(), key=lambda item: item[1], reverse=True)[:10]]\n",
    "#         if 'suicid' in top_10[:3]:\n",
    "#             p += 1\n",
    "        print(i, p, j, rating, doc_id, top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 Results\n",
    "In this case, synonyms and similar terms for military such as 'veteran' and 'medic' could have helped in finding this document.\n",
    "\n",
    "#### MonoT5 Results\n",
    "We notice that the results that Mono T5 finds that are not relevant are related to _some_ of the tokens in the query. For example for the query: \"causes of military suicide\", non-relevant documents that are retrieved often contain the words \"military\" and \"suicide\" but not both. On the other hand, the relevant documents that are not retrieved often contain a lot of synonyms or related words. For the previous query those are words like: \"ptsd\", \"trama\", \"veteran\", \"vietnam\", \"iraq\". Those are words that humans would know are related to military suicides, but such a system would not be aware of that. This is also true for other queries, another interesting example may be \"does legionella pneumophila cause pneumonia\". Here, the model retrieves relevant documents that contains terms like \"legionella\", \"pneumophila\", \"pneumonia\". But it fails to retrieve relevant documents that contain words like: \"bacteria\", \"disease\" and \"organ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Base Query observations. What did the system think were the important terms of the original query, and were they good?\n",
    "\n",
    "#### BM25 Results\n",
    "BM25 has no term weighing? It only removes non-important words.\n",
    "\n",
    "#### MonoT5 Results\n",
    "Is unfortunately not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Expanded Query observations. If the system expanded the query (4 out of 6 systems did), what were the important terms of the expansion, and were they helpful?\n",
    "\n",
    "#### BM25 Results\n",
    "BM25 Uses no expanded queries.\n",
    "\n",
    "#### MonoT5 Results\n",
    "MonoT5 does not expand queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Blunders of system. What obvious mistakes did the system make that it could have easily avoided? Examples might be bad stemming of words or bad handling of hyphenation. Other features of note. Anything else.\n",
    "\n",
    "We can answer this question by looking at the queries with the worst recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-91b74f51b213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrecalls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_recall_per_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mworst_query_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-1e60810100db>\u001b[0m in \u001b[0;36mget_recall_per_query\u001b[0;34m(qrels, results, n)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mrecalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelevant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelevant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrating\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrecalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rank= 5\n",
    "doc = 0\n",
    "\n",
    "recalls = [(k, v) for k, v in get_recall_per_query(qrels, d, 5).items()]\n",
    "recalls.sort(key=lambda x: x[1])\n",
    "worst_query_id = recalls[rank][0]\n",
    "\n",
    "print('Query:', get_query(queries, worst_query_id))\n",
    "print('Recall:', recalls[rank][1])\n",
    "print(tokenize(get_query(queries, worst_query_id)))\n",
    "\n",
    "# relevant = get_relevant_doc_ids_not_retrieved(qrels, worst_query_id, get_ranking_by_query_id(d, worst_query_id))\n",
    "# print(get_doc_vec(relevant[0]))\n",
    "# print(index.doc(str(relevant[doc])).raw())\n",
    "\n",
    "# worst_query_id = recalls[2][0]\n",
    "\n",
    "# print(get_query(queries, worst_query_id))\n",
    "# print(tokenize(get_query(queries, worst_query_id)))\n",
    "\n",
    "# relevant = get_relevant_doc_ids_not_retrieved(qrels, worst_query_id, get_ranking_by_query_id(d, worst_query_id))\n",
    "# print(index.doc(str(relevant[0])).raw())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 Results\n",
    "In this case, the problem with the query with the worst performance is that Percenter is reduced to percent, which now matches with any document using the word percent. This is an obvious stemming issue. This could be resolved by using NER information.\n",
    "\n",
    "In the second query, a spelling mistake prevents good retrieval, and again synonyms for WW1 might be useful.\n",
    "\n",
    "#### MonoT5 Results\n",
    "This model uses the same stemmer as the BM25 model and therefore the same mistake is made with regards to the Percenter --> percent. Also acronyms like \"lps\" are reduced to \"lp\" which yields vastly different results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) What should system to do improve performance? The individual’s conclusion as to why the system did not retrieve well, and recommendations as to what would have made a better retrieval.\n",
    "\n",
    "#### BM25 Results\n",
    "\n",
    "#### MonoT5 Results\n",
    "It is evident that the mistakes that are made are subtle but can have a profounding impact on the results. Some of the common mistakes that have been observed are that documents are found that match only a part of the query. In addition, the tokenization of the queries may remove crucial information from the query. When looking at queries that the model has missed, it seems that these queries often contain terms that are very much related to the query but are not in the query. The model may improve performance if it could find the semantic context of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) What added information would help performance? How can system get that information? Is there implicit information in the query, that a human would understand but the system didn’t? Examples might be world knowledge (like Germany is part of Europe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 Results\n",
    "In general, similar terms and synonyms could benefit greatly in retrieval as queries are often times very small and might miss key terms. Also, a spell checker may benefit search as some queries showed spelling errors which prevents matching of the same intended word. \n",
    "\n",
    "#### MonoT5 Results\n",
    "By using query expansion to extend the query with other relevant terms, the performance could be significantly improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}