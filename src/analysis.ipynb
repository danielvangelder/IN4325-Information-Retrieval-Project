{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6517e324ec9dcf41a34bac95748c49ce676fe8f2d7422e34b5859cd3f859ad06"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions we aim to answer in the analysis\n",
    "# 1) Behavior on top relevant documents [How many of the top documents for this system were relevant and could they be categorized and distinguished from others?]\n",
    "# 2) Behavior on top non-relevant documents [Why were the top non-relevant documents retrieved?] Behavior on unretrieved relevant documents [Why weren’t these relevant documents retrieved within the top 1000?]\n",
    "# x) Beadplot observations [How does the ranking (especially among the top 50 documents) of this system compare to all other systems?]\n",
    "# 3) Base Query observations [What did the system think were the important terms of the original query, and were they good?]\n",
    "# 4) Expanded Query observations [If the system expanded the query (4 out of 6 systems did), what were the important terms of the expansion, and were they helpful?]\n",
    "# 5) Blunders of system [What obvious mistakes did the system make that it could have easily avoided? Examples might be bad stemming of words or bad handling of hyphenation] Other features of note [Anything else.]\n",
    "# 6) What should system to do improve performance? [The individual’s conclusion as to why the system did not retrieve well, and recommendations as to what would have made a better retrieval.]\n",
    "# 7) What added information would help performance? How can system get that information? [Is there implicit information in the query, that a human would understand but the system didn’t? Examples might be world knowledge (like Germany is part of Europe).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reads the output of a model.\n",
    "# Lines in the output should be in the form [query_id, doc_id, rank] with sep = '\\t'.\n",
    "def read_results_tsv(loc):\n",
    "    d = pd.read_csv(loc, sep='\\t', header=None, names=['query_id', 'doc_id', 'rank', 'score'])\n",
    "    return d\n",
    "\n",
    "# Reads the output of a model.\n",
    "# Lines in the output should be in the form [query_id, doc_id, rank] with sep=' '.\n",
    "def read_results_csv(loc):\n",
    "    d = pd.read_csv(loc, sep=' ', header=None, names=['query_id', 'doc_id', 'rank', 'score'])\n",
    "    return d\n",
    "\n",
    "# Gets the ranking of a query.\n",
    "def get_ranking_by_query_id(d, query_id):\n",
    "    ranking = d.loc[d.query_id == query_id][['doc_id', 'rank']].sort_values(by=['rank'])['doc_id'].tolist()\n",
    "    return ranking\n",
    "\n",
    "d = read_results_csv(\"data/bm25-msmarco-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reads the relevant documents from the given qrels file.\n",
    "def read_qrels(loc):\n",
    "    d = pd.read_csv(loc, names=['query_id', 'Q0', 'doc_id', 'rating'], sep=' ', header=None)\n",
    "    del d['Q0']\n",
    "    return d\n",
    "\n",
    "# Gets the relevant document for the given query id.\n",
    "def get_relevant_doc_ids(qrels, query_id):\n",
    "    doc_ids = qrels.loc[(qrels.query_id == query_id)][['doc_id', 'rating']]\n",
    "    return doc_ids\n",
    "\n",
    "# Constructs a vector which counts the number of retrieved documents for each rating.\n",
    "def get_relevance_vector(qrels, results):\n",
    "    v = np.zeros(qrels.rating.max() + 1)\n",
    "    \n",
    "    for query_id in qrels.query_id.unique():\n",
    "        print(query_id)\n",
    "        ranking = get_ranking_by_query_id(d, query_id)\n",
    "        relevant = get_relevant_doc_ids(qrels, query_id)\n",
    "        for doc_id in ranking:\n",
    "            rating = relevant.loc[relevant.doc_id == doc_id]['rating']\n",
    "            v[rating] += 1\n",
    "    return v\n",
    "\n",
    "qrels = read_qrels('../anserini/collections/msmarco-passage/2019qrels-pass.txt')\n",
    "\n",
    "# For some reason, qrels contains less unique query ids, i.e., not every query has relevant items (by a long shot).\n",
    "# print(qrels.query_id.unique())\n",
    "# print(d.query_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Behavior on top relevant documents [How many of the top documents for this system were relevant and could they be categorized and distinguished from others?]\n",
    "print(get_relevance_vector(qrels, d))"
   ]
  }
 ]
}