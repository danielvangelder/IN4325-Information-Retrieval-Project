{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "linear-neighbor",
   "metadata": {},
   "source": [
    "# MS-MARCO Passage (Re-)Ranking using Learn-to-Rank in `pygaggle`\n",
    "This notebook will walk through the basics of learn-to-rank using [pygaggle](https://github.com/castorini/pygaggle).\n",
    "\n",
    "This is the CLI command to run the passage reranking challenge using the Mono T5 algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-executive",
   "metadata": {},
   "source": [
    "```shell\n",
    "python -um pygaggle.run.evaluate_passage_ranker --split dev \\\n",
    "                                                --method t5 \\\n",
    "                                                --model castorini/monot5-base-msmarco \\\n",
    "                                                --dataset collections/msmarco-passage \\\n",
    "                                                --model-type t5-base \\\n",
    "                                                --task msmarco \\\n",
    "                                                --index-dir indexes/msmarco-passage \\\n",
    "                                                --batch-size 32 \\\n",
    "                                                --output-file runs/run.monot5.ans_small.dev.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-scanning",
   "metadata": {},
   "source": [
    "However, we will try to do this in a pythonian manner. First, we import the relevant libraries and the (pretrained) ranking algorithm `MonoT5` ([link?](https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c), [paper](https://arxiv.org/pdf/2003.06713.pdf)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "assumed-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at castorini/monot5-base-msmarco were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from pygaggle.rerank.base import Query, Text\n",
    "from pygaggle.rerank.transformer import MonoT5\n",
    "from pygaggle.rerank.base import hits_to_texts\n",
    "from pyserini.search import SimpleSearcher\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "reranker =  MonoT5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-charles",
   "metadata": {},
   "source": [
    "Now we will import the text from the MS-MARCO dataset and try to re-rank some passages for the queries.\n",
    "\n",
    "**Why re-rank instead of rank?** It is infeasible for a learner to rank all passages in the corpus, therefore we let a conventional retrieval method find a list of ranked _candidates_ that will be re-ranked by the L2R algorithm.\n",
    "\n",
    "We run the example from the pygaggle [Github page](https://github.com/castorini/pygaggle) as a toy problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caroline-publisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank:  1, score: -0.00887, document: {\n",
      "  \"id\" : \"7744105\",\n",
      "  \"contents\" : \"For Earth-centered it was  Geocentric Theory proposed by greeks under the guidance of Ptolemy and Sun-centered was Heliocentric theory proposed by Nicolas Copernicus in 16th century A.D. In short, Your Answers are: 1st blank - Geo-Centric Theory. 2nd blank - Heliocentric Theory.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Here's our query:\n",
    "query = Query('who proposed the geocentric theory')\n",
    "\n",
    "# fetch some passages to rerank from MS MARCO with Pyserini (BM25)\n",
    "searcher = SimpleSearcher('indexes/msmarco-passage/lucene-index-msmarco')\n",
    "hits = searcher.search(query.text)\n",
    "texts = hits_to_texts(hits)\n",
    "\n",
    "# Optionally: print out the passages prior to reranking (might be interesting to see how the order changes):\n",
    "# for i in range(0, 10):\n",
    "#     print(f'{i+1:2} {texts[i].metadata[\"docid\"]:15} {texts[i].score:.5f} {texts[i].text}')\n",
    "\n",
    "# Rerank:\n",
    "reranked = reranker.rerank(query, texts)\n",
    "reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "# Optionally: print out reranked results:\n",
    "# for i in range(0, 10):\n",
    "#     print(f'rank: {i+1:2}, score: {reranked[i].score:.5f}, document: {reranked[i].text}')\n",
    "\n",
    "# We print the first result as a proof of success:\n",
    "print(f'rank: {0+1:2}, score: {reranked[0].score:.5f}, document: {reranked[0].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-cartoon",
   "metadata": {},
   "source": [
    "Toy problems are not sufficient for ranking the whole test set. Therefore, we will try to load all test queries in the dataset ([download link](https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-test2019-queries.tsv.gz)). Make sure to place the test queries file in the `collections/msmarco-passage` directory with the other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "organizational-probe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1108939 what slows down the flow of blood\n",
      "1112389 what is the county for grand rapids, mn\n",
      "792752 what is ruclip\n",
      "1119729 what do you do when you have a nosebleed from having your nose\n",
      "1105095 where is sugar lake lodge located\n",
      "1105103 where is steph currys home in nc\n",
      "1128373 iur definition\n",
      "1127622 meaning of heat capacity\n",
      "1124979 synonym for treatment\n",
      "885490 what party is paul ryan in\n"
     ]
    }
   ],
   "source": [
    "queries = []\n",
    "QUERIES_PATH = 'collections/msmarco-passage/msmarco-test2019-queries.tsv'\n",
    "with open(QUERIES_PATH) as f:\n",
    "    content = f.readlines()\n",
    "    content = [x.strip().split('\\t') for x in content] \n",
    "    queries = [Query(x[1], x[0]) for x in content]\n",
    "for q in queries[:10]:\n",
    "    print(q.id, q.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-activation",
   "metadata": {},
   "source": [
    "Alright, now we are ready to start re-ranking all documents. We should first define a function for outputting a file of the ranked documents (either in `csv` or `tsv` format). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sporting-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_csv(queries, rankings, file_path='runs/monot5.csv'):\n",
    "    '''Desired output format: 'query_id', 'doc_id', 'rank', 'score'\n",
    "    '''\n",
    "    with open(file_path, 'w') as f:\n",
    "        for (i,q) in enumerate(queries):\n",
    "            q_rank = rankings[i]\n",
    "            for (j,r) in enumerate(q_rank):\n",
    "                f.write(str(q.id) + ' ' + str(r.metadata['docid']) + ' ' + str(j) + ' ' + str(r.score) + '\\n')\n",
    "                \n",
    "def output_to_tsv(queries, rankings, file_path='runs/monot5.tsv'):\n",
    "    '''Desired output format: 'query_id', 'doc_id', 'rank', 'score'\n",
    "    '''\n",
    "    with open(file_path, 'w') as f:\n",
    "        for (i,q) in enumerate(queries):\n",
    "            q_rank = rankings[i]\n",
    "            for (j,r) in enumerate(q_rank):\n",
    "                f.write(str(q.id) + '\\t' + str(r.metadata['docid']) + '\\t' + str(j) + ' ' + str(r.score) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-exchange",
   "metadata": {},
   "source": [
    "Given these functions, we are now ready to re-rank all the queries. A script that executes this ranking procedure is given in: `l2r-passage-ranking.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-found",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad71b08b20d847f58a2297749ecac2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform ranking (takes ~15 min.)\n",
    "rankings = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    reranked = reranker.rerank(query, texts)\n",
    "    reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "    rankings.append(reranked)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_csv(queries, rankings)\n",
    "# output_to_tsv(queries, rankings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
