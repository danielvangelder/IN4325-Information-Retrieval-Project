{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "linear-neighbor",
   "metadata": {},
   "source": [
    "# MS-MARCO Passage (Re-)Ranking using Learn-to-Rank in `pygaggle`\n",
    "This notebook will walk through the basics of learn-to-rank using [pygaggle](https://github.com/castorini/pygaggle).\n",
    "\n",
    "This is the CLI command to run the passage reranking challenge using the Mono T5 algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-executive",
   "metadata": {},
   "source": [
    "```shell\n",
    "python -um pygaggle.run.evaluate_passage_ranker --split dev \\\n",
    "                                                --method t5 \\\n",
    "                                                --model castorini/monot5-base-msmarco \\\n",
    "                                                --dataset collections/msmarco-passage \\\n",
    "                                                --model-type t5-base \\\n",
    "                                                --task msmarco \\\n",
    "                                                --index-dir indexes/msmarco-passage \\\n",
    "                                                --batch-size 32 \\\n",
    "                                                --output-file runs/run.monot5.ans_small.dev.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-scanning",
   "metadata": {},
   "source": [
    "However, we will try to do this in a pythonian manner. First, we import the relevant libraries and the (pretrained) ranking algorithm `MonoT5` ([link?](https://towardsdatascience.com/asking-the-right-questions-training-a-t5-transformer-model-on-a-new-task-691ebba2d72c), [paper](https://arxiv.org/pdf/2003.06713.pdf)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygaggle.rerank.base import Query, Text\n",
    "from pygaggle.rerank.transformer import MonoT5\n",
    "from pygaggle.rerank.base import hits_to_texts\n",
    "from pyserini.search import SimpleSearcher\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "reranker =  MonoT5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-charles",
   "metadata": {},
   "source": [
    "Now we will import the text from the MS-MARCO dataset and try to re-rank some passages for the queries.\n",
    "\n",
    "**Why re-rank instead of rank?** It is infeasible for a learner to rank all passages in the corpus, therefore we let a conventional retrieval method find a list of ranked _candidates_ that will be re-ranked by the L2R algorithm.\n",
    "\n",
    "We run the example from the pygaggle [Github page](https://github.com/castorini/pygaggle) as a toy problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's our query:\n",
    "query = Query('who proposed the geocentric theory')\n",
    "\n",
    "# fetch some passages to rerank from MS MARCO with Pyserini (BM25)\n",
    "searcher = SimpleSearcher('indexes/msmarco-passage/lucene-index-msmarco')\n",
    "hits = searcher.search(query.text)\n",
    "texts = hits_to_texts(hits)\n",
    "\n",
    "# Optionally: print out the passages prior to reranking (might be interesting to see how the order changes):\n",
    "# for i in range(0, 10):\n",
    "#     print(f'{i+1:2} {texts[i].metadata[\"docid\"]:15} {texts[i].score:.5f} {texts[i].text}')\n",
    "\n",
    "# Rerank:\n",
    "reranked = reranker.rerank(query, texts)\n",
    "reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "# Optionally: print out reranked results:\n",
    "# for i in range(0, 10):\n",
    "#     print(f'rank: {i+1:2}, score: {reranked[i].score:.5f}, document: {reranked[i].text}')\n",
    "\n",
    "# We print the first result as a proof of success:\n",
    "print(f'rank: {0+1:2}, score: {reranked[0].score:.5f}, document: {reranked[0].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-cartoon",
   "metadata": {},
   "source": [
    "Toy problems are not sufficient for ranking the whole test set. Therefore, we will try to load all test queries in the dataset ([download link](https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-test2019-queries.tsv.gz)). Make sure to place the test queries file in the `collections/msmarco-passage` directory with the other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "QUERIES_PATH = 'collections/msmarco-passage/msmarco-test2019-queries.tsv'\n",
    "with open(QUERIES_PATH) as f:\n",
    "    content = f.readlines()\n",
    "    content = [x.strip().split('\\t') for x in content] \n",
    "    queries = [Query(x[1], x[0]) for x in content]\n",
    "for q in queries[:10]:\n",
    "    print(q.id, q.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-thing",
   "metadata": {},
   "source": [
    "Alright, now we are ready to start re-ranking all documents. We should first define a function for outputting a file of the ranked documents (either in `csv` or `tsv` format). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_csv(queries, rankings, file_path='runs/monot5.csv'):\n",
    "    '''Desired output format: 'query_id', 'doc_id', 'rank', 'score'\n",
    "    '''\n",
    "    with open(file_path, 'w') as f:\n",
    "        for (i,q) in enumerate(queries):\n",
    "            q_rank = rankings[i]\n",
    "            for (j,r) in enumerate(q_rank):\n",
    "                f.write(str(q.id) + ' ' + str(r.metadata['docid']) + ' ' + str(j) + ' ' + str(r.score) + '\\n')\n",
    "                \n",
    "def output_to_tsv(queries, rankings, file_path='runs/monot5.tsv'):\n",
    "    '''Desired output format: 'query_id', 'doc_id', 'rank', 'score'\n",
    "    '''\n",
    "    with open(file_path, 'w') as f:\n",
    "        for (i,q) in enumerate(queries):\n",
    "            q_rank = rankings[i]\n",
    "            for (j,r) in enumerate(q_rank):\n",
    "                f.write(str(q.id) + '\\t' + str(r.metadata['docid']) + '\\t' + str(j) + ' ' + str(r.score) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-coating",
   "metadata": {},
   "source": [
    "Given these functions, we are now ready to re-rank all the queries. A script that executes this ranking procedure is given in: `l2r-passage-ranking.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ranking (takes ~15 min.)\n",
    "rankings = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    reranked = reranker.rerank(query, texts)\n",
    "    reranked.sort(key=lambda x: x.score, reverse=True)\n",
    "    rankings.append(reranked)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_csv(queries, rankings)\n",
    "# output_to_tsv(queries, rankings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
